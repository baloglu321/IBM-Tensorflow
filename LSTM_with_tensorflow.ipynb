{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5110bfce-6460-4f20-bc81-6ddd6086edb6",
   "metadata": {},
   "source": [
    "<h2> Sequential Data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b5ef0d-2067-4f77-bb1e-461ee6d395f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whenever the points in a dataset are dependent on the other points, \n",
    "#the data is said to be sequential. \n",
    "#A common example of this is a time series, such as a stock price, \n",
    "#or sensor data, where each data point represents an observation at a certain point in time. \n",
    "#There are other examples of sequential data, like sentences, gene sequences, and weather data. \n",
    "#But traditional neural networks typically can't handle this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8aab57-cb52-42b3-859a-001f161bbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507ebd8c-c92e-4821-8ad7-b4e1ec01f791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n",
    "\n",
    "state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0783c2b-8d91-4612-898e-a97c31026823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "lstm = tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)\n",
    "\n",
    "lstm.states=state\n",
    "\n",
    "print(lstm.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fed4cc-3e2c-4535-b85f-ebcfadebf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n",
    "\n",
    "batch_size = 1\n",
    "sentence_max_length = 1\n",
    "n_features = 6\n",
    "\n",
    "new_shape = (batch_size, sentence_max_length, n_features)\n",
    "\n",
    "inputs = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0a08b3-9672-4181-a36c-8839b27f21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :  tf.Tensor([1 1 4], shape=(3,), dtype=int32)\n",
      "Memory :  tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
      "Carry state :  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "print('Output : ', tf.shape(output))\n",
    "\n",
    "print('Memory : ',tf.shape(final_memory_state))\n",
    "\n",
    "print('Carry state : ',tf.shape(final_carry_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e42af3d-955e-43d2-878b-7681a3e0c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []\n",
    "LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n",
    "cell1 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_1)\n",
    "cells.append(cell1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daaba78e-8b53-4179-9e88-176a11b60b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n",
    "cell2 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_2)\n",
    "cells.append(cell2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40afa6e1-ca3c-4c0c-927d-9c3d89778f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm =  tf.keras.layers.StackedRNNCells(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2eca880-0c6c-4ecf-a33c-f41924a6a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer= tf.keras.layers.RNN(stacked_lstm ,return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "681508b5-5b9e-465d-b402-202704119eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch size x time steps x features.\n",
    "sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n",
    "sample_input\n",
    "\n",
    "batch_size = 2\n",
    "time_steps = 3\n",
    "features = 6\n",
    "new_shape = (batch_size, time_steps, features)\n",
    "\n",
    "x = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ac92c1-e195-45c9-ab48-15a00181be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, final_memory_state, final_carry_state  = lstm_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29041277-9439-4974-a03a-fba128dd1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :  tf.Tensor([2 3 5], shape=(3,), dtype=int32)\n",
      "Memory :  tf.Tensor([2 2 4], shape=(3,), dtype=int32)\n",
      "Carry state :  tf.Tensor([2 2 5], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('Output : ', tf.shape(output))\n",
    "\n",
    "print('Memory : ',tf.shape(final_memory_state))\n",
    "\n",
    "print('Carry state : ',tf.shape(final_carry_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6684c22-1bf7-44fb-b0c6-8d8d8c5274fd",
   "metadata": {},
   "source": [
    "<h2> Language Modelling with LSTM </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fe95645-fd39-4d35-9939-a4645111271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00e3d55-477d-4440-a951-b31b76ec9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "  with tf.io.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary, word_to_id\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ebd8e1d-fde3-49c2-a1c3-2b83a810516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 30\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size= 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5b105-cd73-45ba-b6de-a175d6182f96",
   "metadata": {},
   "source": [
    "<h3>Training data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c863461-8fec-4223-afde-2f3ba88322dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85865af5-a9c5-4ab6-a3da-b5cdb66f32b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef6b480-9639-4db8-9955-5cf0b1f8dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1abffb56-275c-4a9a-9a69-625f51eb2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "_input_data = first_touple[0]\n",
    "_targets = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deca625a-106f-4ddf-a75f-0e03d4c4782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96d230c4-1635-4982-b9c0-7d526964a900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdd80fc8-f430-4922-a165-952b8f6049ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
       "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
       "          12,    3,   21,    2, 2915,    2,   12,    3,   21]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e621d2d1-eada-45e7-a8b8-00fe23cf0dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word(_input_data[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1e30-15ce-43cf-8caf-d31d031f4742",
   "metadata": {},
   "source": [
    "<h3>Embeddings</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30246d64-cbc5-4d4d-a712-8e47a5a280b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size,batch_input_shape=(batch_size, num_steps),trainable=True,name=\"embedding_vocab\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45a15d72-0091-4e30-b848-b9ec0b82644c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 200), dtype=float32, numpy=\n",
       "array([[[ 1.36911981e-02, -2.38683354e-02, -2.19960343e-02, ...,\n",
       "          1.51716806e-02, -4.15931456e-02,  1.50777586e-02],\n",
       "        [-1.83446296e-02, -5.49820811e-03,  4.47747596e-02, ...,\n",
       "         -3.41062695e-02,  4.88087572e-02, -1.40839815e-03],\n",
       "        [ 2.16720365e-02, -1.56941898e-02,  3.83081324e-02, ...,\n",
       "         -4.51611765e-02, -4.30250429e-02, -4.79110740e-02],\n",
       "        ...,\n",
       "        [-3.41730937e-02,  4.38601263e-02, -3.62863392e-03, ...,\n",
       "          4.47701328e-02, -3.30872089e-02,  3.58760357e-03],\n",
       "        [-8.66303593e-03, -2.98999902e-02,  6.04574755e-03, ...,\n",
       "          3.46110575e-02, -2.39387639e-02, -2.97568794e-02],\n",
       "        [ 9.56581905e-03,  3.47135551e-02,  4.14527319e-02, ...,\n",
       "          6.00447506e-03, -4.74563241e-02,  4.99121435e-02]],\n",
       "\n",
       "       [[-1.27938166e-02, -2.87154913e-02, -3.10521964e-02, ...,\n",
       "          2.19414271e-02, -4.46020365e-02, -2.59881616e-02],\n",
       "        [ 2.17729472e-02, -3.60244513e-02, -2.90478598e-02, ...,\n",
       "          2.66485251e-02, -2.60492917e-02, -4.33641188e-02],\n",
       "        [-2.09362041e-02,  1.53473355e-02,  2.65023001e-02, ...,\n",
       "          4.55026962e-02, -1.38981938e-02,  5.95120341e-03],\n",
       "        ...,\n",
       "        [-4.72626351e-02, -1.23465434e-02, -4.67707627e-02, ...,\n",
       "          8.65356997e-03, -4.92696054e-02, -1.54203661e-02],\n",
       "        [-3.29121128e-02, -2.34686621e-02, -4.10587080e-02, ...,\n",
       "         -2.02681180e-02,  1.22982040e-02,  6.75775111e-04],\n",
       "        [-2.84822714e-02,  6.90650195e-04,  2.17852704e-02, ...,\n",
       "         -8.03603977e-03,  2.38872692e-03, -9.95247439e-03]],\n",
       "\n",
       "       [[-4.95417602e-02,  4.55112383e-03, -3.91660333e-02, ...,\n",
       "          2.35180371e-02, -2.80664209e-02,  3.50517295e-02],\n",
       "        [ 4.34059538e-02, -3.29809561e-02,  4.68379371e-02, ...,\n",
       "          1.18241459e-03, -2.15096716e-02,  3.45338620e-02],\n",
       "        [ 3.46461050e-02,  2.90556587e-02, -1.21569745e-02, ...,\n",
       "          1.16605647e-02, -4.26040962e-03,  7.61948898e-03],\n",
       "        ...,\n",
       "        [-4.43360321e-02,  2.46694349e-02, -1.16916075e-02, ...,\n",
       "          3.85984033e-03,  2.25032456e-02,  2.96796001e-02],\n",
       "        [ 2.59377472e-02,  2.39969604e-02, -1.72368437e-03, ...,\n",
       "          3.45644616e-02, -1.73579939e-02, -2.42752191e-02],\n",
       "        [ 3.45579870e-02, -2.87708044e-02,  3.06349434e-02, ...,\n",
       "          4.13628258e-02,  4.45890911e-02,  9.98405367e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.01296160e-03,  5.13827801e-03, -1.11791268e-02, ...,\n",
       "         -6.90116733e-03,  5.17605618e-03, -4.78655584e-02],\n",
       "        [ 9.50552523e-04, -2.09713113e-02, -2.86093485e-02, ...,\n",
       "         -4.05116901e-02, -1.02218240e-03,  2.99362056e-02],\n",
       "        [ 4.52766754e-02,  2.19765343e-02,  4.15342189e-02, ...,\n",
       "          4.27278988e-02, -1.60731003e-03,  3.87487896e-02],\n",
       "        ...,\n",
       "        [ 2.08315589e-02,  2.11669840e-02,  1.94514506e-02, ...,\n",
       "         -3.86013761e-02,  4.25972603e-02, -1.42558701e-02],\n",
       "        [-3.26766148e-02,  1.34325661e-02,  3.70994955e-03, ...,\n",
       "         -4.32641767e-02, -4.29363847e-02,  1.76617615e-02],\n",
       "        [-5.56766987e-04,  3.76862548e-02,  8.21875408e-03, ...,\n",
       "         -3.64972241e-02, -1.64177790e-02,  1.93773769e-02]],\n",
       "\n",
       "       [[ 1.90556049e-04, -3.01496740e-02, -3.93922217e-02, ...,\n",
       "          3.37135792e-03, -7.54135847e-03,  4.72454764e-02],\n",
       "        [ 1.65980943e-02,  3.50232758e-02,  5.09558991e-03, ...,\n",
       "         -4.40729782e-03, -3.73486280e-02, -4.19351459e-02],\n",
       "        [ 3.46461050e-02,  2.90556587e-02, -1.21569745e-02, ...,\n",
       "          1.16605647e-02, -4.26040962e-03,  7.61948898e-03],\n",
       "        ...,\n",
       "        [-1.30466223e-02, -3.22095640e-02, -5.58137894e-04, ...,\n",
       "         -1.71598196e-02, -2.35945117e-02,  2.02313997e-02],\n",
       "        [ 2.17729472e-02, -3.60244513e-02, -2.90478598e-02, ...,\n",
       "          2.66485251e-02, -2.60492917e-02, -4.33641188e-02],\n",
       "        [ 8.49284232e-04,  1.79251917e-02,  3.28562409e-03, ...,\n",
       "          2.73151509e-02,  2.20624842e-02, -3.88806835e-02]],\n",
       "\n",
       "       [[-3.78347747e-02, -4.97000925e-02,  3.95503901e-02, ...,\n",
       "         -1.02557428e-02,  8.63770396e-03, -6.82257488e-03],\n",
       "        [-1.79216154e-02, -3.90021577e-02,  1.99174322e-02, ...,\n",
       "         -2.57671829e-02,  3.86219658e-02,  3.26559693e-03],\n",
       "        [ 4.08601500e-02, -2.93195248e-03, -7.88317993e-03, ...,\n",
       "         -3.87476459e-02,  1.96977705e-03,  3.63719128e-02],\n",
       "        ...,\n",
       "        [ 1.58162042e-03,  9.74759459e-05,  4.61002477e-02, ...,\n",
       "          6.89197704e-03, -8.20197910e-03,  3.77224423e-02],\n",
       "        [-1.23608336e-02,  1.35490634e-02, -4.16732952e-03, ...,\n",
       "          4.99329232e-02, -9.99038294e-03,  1.37944110e-02],\n",
       "        [ 3.39577086e-02,  3.15181874e-02, -4.15222645e-02, ...,\n",
       "          3.81083004e-02, -1.34248510e-02, -9.38909128e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = embedding_layer(_input_data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125ec0e-2f6a-4985-be89-2fbab726d285",
   "metadata": {},
   "source": [
    "<h3>Constructing Recurrent Neural Networks</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bca8c6a3-78db-4e39-afb3-952fa9e4b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbe372b7-3859-47e8-bd6b-b3030d71ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "347385d9-edc7-431b-a8bd-fd11ad88730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5529473a-5c71-4a4f-9fa2-e1c852658c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(30, 200) dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
    "layer.inital_state = init_state\n",
    "layer.inital_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501f464c-bcb8-4ed3-9347-6d6ac99bd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "321b9e0d-4e4f-4330-b0bf-2d0e71e8a0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 128), dtype=float32, numpy=\n",
       "array([[[ 5.23586641e-04,  6.66461885e-04,  7.64806755e-04, ...,\n",
       "          7.89060374e-04,  1.33517559e-03,  3.96889308e-03],\n",
       "        [ 9.63843719e-04,  9.25735512e-04,  2.12256913e-03, ...,\n",
       "          9.33756586e-04,  2.70738266e-03,  4.99183172e-03],\n",
       "        [ 2.85160565e-03,  1.85258593e-03,  3.33827827e-03, ...,\n",
       "          1.73360389e-03,  1.08783878e-03,  5.39264642e-03],\n",
       "        ...,\n",
       "        [-5.67265693e-03,  1.32962107e-03, -1.46105897e-03, ...,\n",
       "          3.28588742e-03, -2.80904211e-03, -2.55321997e-04],\n",
       "        [-5.12129255e-03, -1.17917603e-03, -7.77002075e-04, ...,\n",
       "          2.55897036e-03, -2.77340878e-03, -7.28090061e-04],\n",
       "        [-4.07692604e-03, -2.55182153e-03, -4.61588323e-04, ...,\n",
       "          3.12436568e-05, -3.02291336e-03, -7.73620617e-04]],\n",
       "\n",
       "       [[ 1.89789222e-04, -6.21110841e-04, -1.60401524e-03, ...,\n",
       "          1.10104425e-04, -1.28551561e-03, -6.43569161e-04],\n",
       "        [ 7.80227128e-04, -1.42132782e-03, -2.68910220e-03, ...,\n",
       "          2.99474137e-04, -1.69456203e-03,  1.00656784e-04],\n",
       "        [ 1.26131927e-03, -2.03162036e-03, -2.43055588e-03, ...,\n",
       "          8.09029443e-04, -3.51258437e-03,  2.46652961e-03],\n",
       "        ...,\n",
       "        [ 1.14891585e-03,  5.72348107e-03, -3.93817481e-03, ...,\n",
       "         -4.86055203e-03, -2.98393425e-03, -8.25298252e-04],\n",
       "        [ 2.13276944e-03,  7.76131218e-03, -6.71794871e-03, ...,\n",
       "         -4.75578662e-03, -2.00915732e-03,  1.10233529e-03],\n",
       "        [ 3.90362530e-03,  8.72317795e-03, -7.80558074e-03, ...,\n",
       "         -3.38361342e-03, -1.33215697e-04,  2.15475168e-03]],\n",
       "\n",
       "       [[-9.73632734e-04,  3.15479061e-04, -3.96136573e-04, ...,\n",
       "         -1.52432977e-03,  1.22428848e-03,  7.00510864e-04],\n",
       "        [-1.87668437e-03,  3.27847083e-04, -1.65775453e-03, ...,\n",
       "         -1.35276432e-03,  1.37380592e-03, -5.52472251e-04],\n",
       "        [-2.48335279e-03, -1.06657424e-03, -1.48276612e-03, ...,\n",
       "         -8.59113759e-04,  1.33110420e-03, -1.57795951e-03],\n",
       "        ...,\n",
       "        [-5.67172188e-03,  1.91137020e-03, -8.86003673e-03, ...,\n",
       "          4.05461527e-03,  6.37754006e-03, -3.14807723e-04],\n",
       "        [-5.26722660e-03,  2.67251488e-03, -7.55907362e-03, ...,\n",
       "          3.78583162e-03,  5.74611127e-03,  4.05965984e-04],\n",
       "        [-5.31840324e-03,  2.02359678e-03, -5.82636567e-03, ...,\n",
       "          3.05280834e-03,  5.48204780e-03,  1.63941539e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.31231465e-03, -1.45727361e-04,  5.67838899e-04, ...,\n",
       "          1.50683391e-05, -9.08402362e-05, -1.92750304e-03],\n",
       "        [ 1.58522977e-03, -8.06610624e-04,  2.76583625e-04, ...,\n",
       "          3.01526627e-04, -8.36343737e-04, -1.44501438e-03],\n",
       "        [ 1.49650231e-03, -1.11788348e-03,  8.94229801e-04, ...,\n",
       "          1.16630411e-03, -2.00089789e-03, -6.95910887e-04],\n",
       "        ...,\n",
       "        [ 2.54377117e-03,  6.99528027e-04, -2.33907741e-03, ...,\n",
       "         -1.98164931e-03,  7.14027276e-03, -2.34815548e-03],\n",
       "        [ 1.75731652e-03,  4.28895408e-04, -3.63186537e-03, ...,\n",
       "         -2.37100548e-03,  6.17806846e-03, -5.35583682e-03],\n",
       "        [ 1.13794010e-03,  5.35684994e-05, -2.97339587e-03, ...,\n",
       "         -1.77944463e-03,  4.65840055e-03, -8.17048363e-03]],\n",
       "\n",
       "       [[-9.42991639e-04, -4.17805539e-04, -1.22757323e-04, ...,\n",
       "          4.67080477e-04,  1.05119857e-03, -5.05784883e-05],\n",
       "        [-1.65918272e-03,  1.28161235e-04, -1.85293437e-03, ...,\n",
       "          1.33834954e-03,  5.21710259e-04, -4.29563166e-04],\n",
       "        [-2.50380789e-03, -1.15749205e-03, -1.61329308e-03, ...,\n",
       "          2.74177897e-03,  2.71537690e-04, -1.17242348e-03],\n",
       "        ...,\n",
       "        [-1.27265649e-03,  1.81381649e-03,  3.73317674e-03, ...,\n",
       "         -4.20911529e-04, -9.11928480e-04,  3.06397374e-03],\n",
       "        [-1.37201580e-03,  8.34226725e-04,  3.63624864e-03, ...,\n",
       "         -2.88071606e-04, -1.77159416e-03,  3.65707744e-03],\n",
       "        [-6.96799834e-04, -4.58334194e-04,  4.09134896e-03, ...,\n",
       "         -1.61254162e-03, -1.81125884e-03,  2.91069061e-03]],\n",
       "\n",
       "       [[-3.16647463e-04, -3.04566231e-04, -5.32475009e-04, ...,\n",
       "         -5.42459893e-04,  1.18997879e-03, -7.24697020e-04],\n",
       "        [-2.61623319e-03, -1.74867758e-03, -7.41152151e-04, ...,\n",
       "         -3.72327777e-04,  1.13162317e-03, -7.40974574e-05],\n",
       "        [-3.91337322e-03, -2.65834411e-03,  6.16590478e-06, ...,\n",
       "         -8.10310477e-04,  8.31813726e-04,  2.00410650e-04],\n",
       "        ...,\n",
       "        [-8.22263211e-03, -1.42242655e-03,  2.42830953e-03, ...,\n",
       "         -9.58778837e-04, -1.95025664e-03,  8.83815810e-04],\n",
       "        [-7.98888039e-03, -6.38999103e-04,  1.27074949e-03, ...,\n",
       "         -1.85364275e-04, -2.62644445e-03,  2.73968116e-03],\n",
       "        [-7.06690457e-03, -4.06118575e-04,  2.63268070e-04, ...,\n",
       "          1.41723664e-03, -1.88061444e-03,  3.72181321e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26543e86-ecdc-4347-bbab-f632fd556a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from dense layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "dense = tf.keras.layers.Dense(vocab_size)\n",
    "logits_outputs  = dense(outputs)\n",
    "print(\"shape of the output from dense layer: \", logits_outputs.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e3a443f-b8f3-4f76-bbc6-c2aa3e7cc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from the activation layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "activation = tf.keras.layers.Activation('softmax')\n",
    "output_words_prob = activation(logits_outputs)\n",
    "print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fde2f6f-f198-428f-944a-ea107e217d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of observing words in t=0 to t=20 tf.Tensor(\n",
      "[[9.99876211e-05 9.99798140e-05 1.00015728e-04 ... 9.99814074e-05\n",
      "  1.00037185e-04 1.00000878e-04]\n",
      " [9.99777694e-05 9.99483891e-05 9.99934055e-05 ... 9.99705298e-05\n",
      "  1.00041085e-04 1.00000230e-04]\n",
      " [9.99718250e-05 9.99314288e-05 9.99816402e-05 ... 9.99526455e-05\n",
      "  1.00029283e-04 1.00018777e-04]\n",
      " ...\n",
      " [1.00086305e-04 1.00027224e-04 9.99926779e-05 ... 1.00098085e-04\n",
      "  1.00035366e-04 1.00035220e-04]\n",
      " [1.00096993e-04 1.00022444e-04 9.99803015e-05 ... 1.00103214e-04\n",
      "  1.00022247e-04 1.00036690e-04]\n",
      " [1.00109282e-04 1.00009624e-04 9.99919939e-05 ... 1.00076621e-04\n",
      "  1.00036203e-04 1.00037556e-04]], shape=(20, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd2055ad-a79d-4a24-b9eb-efdb397651ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8706,  824, 9585, 9585, 9585, 6578, 5527, 5531, 5531, 1211, 1211,\n",
       "        131,  131,  131,  131, 9184, 9184, 6437, 5638, 5638], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0,0:num_steps], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc8205e0-ad75-419b-9bdc-e284b709ad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a32e8c9-294a-4059-ae13-813894936f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b595928d-5816-41cf-815e-85e3beeede53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss  = crossentropy(_targets, output_words_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "057d0be8-69a1-4b89-bb88-ae6fd7c9d58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([9.210808, 9.210276, 9.210986, 9.210007, 9.210913, 9.210234,\n",
       "       9.209206, 9.210795, 9.209507, 9.21038 ], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac0d17f6-847b-4b24-bdc9-dc5d1e1939c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=184.20636>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss / batch_size)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03eacc3c-8197-4021-adc7-792a56ee4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, clipnorm=max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93a72b40-31ac-4cc4-85fe-8d0f018364fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_vocab (Embedding)  (30, 20, 200)            2000000   \n",
      "                                                                 \n",
      " rnn_1 (RNN)                 (30, 20, 128)             671088    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (30, 20, 10000)           1290000   \n",
      "                                                                 \n",
      " activation (Activation)     (30, 20, 10000)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layer)\n",
    "model.add(dense)\n",
    "model.add(activation)\n",
    "model.compile(loss=crossentropy, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6ea3302-3d52-4c8b-8a37-2c3e5c46bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8870be3b-93d7-4039-8bfd-b506f2076073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab/embeddings:0',\n",
       " 'rnn_1/stacked_rnn_cells_1/lstm_cell_3/kernel:0',\n",
       " 'rnn_1/stacked_rnn_cells_1/lstm_cell_3/recurrent_kernel:0',\n",
       " 'rnn_1/stacked_rnn_cells_1/lstm_cell_3/bias:0',\n",
       " 'rnn_1/stacked_rnn_cells_1/lstm_cell_4/kernel:0',\n",
       " 'rnn_1/stacked_rnn_cells_1/lstm_cell_4/recurrent_kernel:0',\n",
       " 'rnn_1/stacked_rnn_cells_1/lstm_cell_4/bias:0',\n",
       " 'dense_1/kernel:0',\n",
       " 'dense_1/bias:0']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "966c0ce9-703c-469e-b60b-8bb09d259e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(1.0)\n",
    "y =  tf.constant(2.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    g.watch(y)\n",
    "    func_test = 2 * x * x + 3 * x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa5b9e1f-0264-4820-ac02-398bea3f13cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_grad = g.gradient(func_test, x) # Will compute to 10.0\n",
    "print(var_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44c99ca4-f879-46ef-afea-4591a5376431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_grad = g.gradient(func_test, y) # Will compute to 3.0\n",
    "print(var_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef38c2fc-532c-4987-93f7-602d1d527936",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass.\n",
    "    output_words_prob = model(_input_data)\n",
    "    # Loss value for this batch.\n",
    "    loss  = crossentropy(_targets, output_words_prob)\n",
    "    cost = tf.reduce_sum(loss,axis=0) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28e60ff7-e6f4-4cf1-bc07-c63fdc2cfa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gradients of loss wrt the trainable variables.\n",
    "grad_t_list = tape.gradient(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9c6aac4-4978-4cde-86ea-2b716d232932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x000001CCE0FA9990>, <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
      "array([[-1.21057360e-06,  1.11048939e-06,  1.81532386e-07, ...,\n",
      "        -9.68049108e-08, -3.67111710e-08, -2.76418064e-07],\n",
      "       [-5.07294317e-07,  1.44252951e-07, -3.60630679e-07, ...,\n",
      "         2.64155318e-07, -3.07675350e-07, -2.30252766e-07],\n",
      "       [ 7.60647367e-08,  4.53663773e-07,  2.49056058e-07, ...,\n",
      "         3.81014900e-08, -1.46729548e-07, -2.64645848e-07],\n",
      "       ...,\n",
      "       [-2.80405317e-07,  4.15165147e-08, -7.11877249e-07, ...,\n",
      "        -1.83229218e-07,  2.98460407e-07, -1.07527256e-07],\n",
      "       [-1.00541524e-06,  8.35447622e-09,  3.09439770e-07, ...,\n",
      "         3.70392456e-07,  8.74839259e-08, -1.75751911e-08],\n",
      "       [ 5.03668559e-07, -2.07654324e-07,  5.58799741e-07, ...,\n",
      "        -3.52775515e-08,  8.56571276e-08,  1.86711205e-07]], dtype=float32)>, <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
      "array([[-9.39754159e-08,  1.20600305e-07,  1.49682549e-07, ...,\n",
      "         7.05727476e-09,  1.08695993e-07, -5.68424241e-08],\n",
      "       [-1.25676607e-07, -2.38073767e-07,  1.11631017e-07, ...,\n",
      "        -8.36142675e-08,  5.11769187e-08,  2.35598208e-07],\n",
      "       [ 8.45096082e-08, -2.46632766e-07, -1.70544027e-07, ...,\n",
      "        -1.34838075e-07,  1.55118187e-07, -2.45559946e-08],\n",
      "       ...,\n",
      "       [ 8.54810196e-08, -2.42944964e-07, -5.53831434e-08, ...,\n",
      "         2.32535896e-07, -1.89823268e-08, -6.50923866e-08],\n",
      "       [-5.12402849e-08, -1.50184221e-09,  2.83389909e-07, ...,\n",
      "        -2.23845564e-08, -9.54893551e-08, -1.48459435e-07],\n",
      "       [-7.75154660e-08, -1.17515718e-07, -1.05618732e-07, ...,\n",
      "        -1.15903156e-07, -2.29969039e-08, -3.15501325e-07]], dtype=float32)>, <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
      "array([-5.38379027e-07,  1.60316449e-05, -3.12425982e-05, ...,\n",
      "       -7.08450443e-07,  9.99760960e-06,  1.23730915e-05], dtype=float32)>, <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
      "array([[ 6.06070785e-08,  6.87455923e-08,  2.43316549e-07, ...,\n",
      "         5.35215349e-07, -3.28748797e-08,  2.62320782e-07],\n",
      "       [ 3.66052689e-07, -2.42609701e-08, -2.37313643e-07, ...,\n",
      "        -3.54075610e-07, -5.13681009e-08,  2.19753176e-08],\n",
      "       [-1.98026875e-08,  9.22471060e-08, -3.30909074e-07, ...,\n",
      "        -4.02424860e-07,  6.62367938e-10, -1.78026710e-07],\n",
      "       ...,\n",
      "       [-1.71099188e-07,  7.63392194e-08,  3.08997471e-07, ...,\n",
      "         2.63024077e-07, -4.37670451e-08,  1.64783188e-07],\n",
      "       [ 5.45193828e-08,  9.91540077e-08,  1.17502260e-07, ...,\n",
      "         4.08629091e-07, -1.11947024e-07, -8.39211296e-08],\n",
      "       [-1.13241867e-07,  3.20164943e-08, -6.80008583e-08, ...,\n",
      "         1.40893292e-07, -8.66539622e-08,  1.48451704e-07]], dtype=float32)>, <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
      "array([[-2.83014316e-07,  1.87468714e-07,  1.28782503e-07, ...,\n",
      "         3.23095236e-07,  1.57001807e-08,  1.13150683e-07],\n",
      "       [ 7.05473440e-08,  4.75358178e-07, -1.00234580e-07, ...,\n",
      "        -6.71451801e-08,  9.61120747e-08,  2.60390998e-08],\n",
      "       [-1.18739919e-07, -5.65627190e-08,  4.37331437e-07, ...,\n",
      "         3.94034771e-07, -1.19753935e-07,  1.44101236e-07],\n",
      "       ...,\n",
      "       [-1.48003664e-07,  1.25589743e-08,  1.12538693e-07, ...,\n",
      "         8.71929615e-07,  4.78658535e-08,  1.13756116e-07],\n",
      "       [-5.59820563e-08, -2.40167680e-07,  3.46549811e-09, ...,\n",
      "         1.33972321e-07,  2.50653954e-07,  5.63789122e-08],\n",
      "       [ 1.24898776e-07, -4.92950925e-08,  3.87349459e-08, ...,\n",
      "         1.02594122e-07, -7.39265289e-08,  3.15293335e-07]], dtype=float32)>, <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
      "array([ 2.49469322e-05,  7.57677481e-06, -6.18866834e-05,  7.69003145e-06,\n",
      "       -2.01745715e-05, -7.07551080e-05,  2.34803110e-06, -3.37810634e-05,\n",
      "       -1.83314169e-05,  1.60549462e-07, -2.85602018e-05, -3.31013871e-05,\n",
      "        1.68080423e-05,  5.80545093e-05,  1.67938597e-05,  3.17677186e-05,\n",
      "        4.10325883e-05,  3.74359206e-06, -6.53401912e-07,  2.36837186e-05,\n",
      "       -3.87026193e-05,  2.80141649e-05,  3.28906317e-05, -6.07566471e-06,\n",
      "        7.30706779e-06, -1.52821212e-05, -6.35160395e-05,  9.47938097e-05,\n",
      "       -3.03235865e-05,  7.59836757e-06,  1.56651367e-06,  1.06875068e-05,\n",
      "       -1.54031641e-05, -3.18363982e-06,  2.58625005e-05,  4.22712765e-05,\n",
      "       -5.93205186e-06,  4.68410217e-05, -7.09049782e-05,  2.50994854e-05,\n",
      "        1.33377107e-05,  3.60924241e-06, -2.23639072e-05,  1.08682696e-04,\n",
      "       -5.17777808e-05,  2.59191656e-05, -4.63316701e-06,  4.23153142e-05,\n",
      "        5.39548546e-06,  2.84113571e-06,  1.48204444e-05,  2.45996398e-06,\n",
      "       -1.94262720e-05, -5.53641294e-05, -4.84403063e-05,  4.65652120e-05,\n",
      "       -6.85445339e-05,  2.23281277e-05, -2.61072819e-05, -1.11864410e-05,\n",
      "        2.71094577e-05, -1.33286103e-05, -9.72013822e-06, -1.57511931e-05,\n",
      "        2.42388487e-05, -5.52007186e-05,  4.92288382e-06,  1.43906227e-05,\n",
      "        1.09953053e-05,  2.31760569e-05,  3.18352395e-05,  9.57579960e-06,\n",
      "        1.34406309e-06,  3.28085080e-05, -1.70384956e-05, -3.86976080e-06,\n",
      "       -2.47961798e-05,  1.48823556e-05,  2.04836579e-05, -3.22020278e-05,\n",
      "       -1.08057520e-05,  7.62266864e-05,  2.59039571e-05,  1.29705195e-05,\n",
      "       -2.37094009e-05, -6.50939328e-05, -1.44419664e-06,  3.54880976e-05,\n",
      "        5.64080074e-05, -6.43202975e-06, -1.27343619e-05, -3.32722157e-05,\n",
      "        4.81199750e-05, -3.57114950e-05,  2.01606599e-05, -7.62829586e-05,\n",
      "        1.94226550e-06, -2.14329430e-05,  7.00861892e-06, -2.62525646e-07,\n",
      "       -5.66908966e-06,  2.22185663e-05, -5.50520563e-05, -3.14446806e-05,\n",
      "       -1.64065423e-05, -2.08175243e-06, -2.38712328e-05, -8.86377984e-06,\n",
      "        1.11705667e-05,  2.63177280e-05,  6.36487630e-06,  4.42090422e-05,\n",
      "       -1.72216387e-05,  8.35312221e-06, -1.43471034e-05,  1.11307145e-05,\n",
      "       -1.02987560e-05, -2.53891158e-05, -4.38444076e-05, -1.86000925e-05,\n",
      "        3.74551673e-05,  6.43037765e-06,  3.15131729e-07,  1.33504418e-05,\n",
      "        5.33794264e-06, -7.09733649e-05,  1.81977375e-05, -4.61492746e-05,\n",
      "        1.75672467e-05, -1.43281841e-05, -8.89812072e-05,  1.67135331e-05,\n",
      "       -6.21683212e-05, -1.09721732e-04, -6.30143859e-07, -4.53004468e-05,\n",
      "       -4.66381898e-05, -2.40990175e-07, -2.93775047e-05, -5.48587777e-05,\n",
      "        2.34140953e-05,  8.02775903e-05,  5.00582928e-06,  3.48928806e-05,\n",
      "        5.81128479e-05,  1.27842359e-05, -1.47568135e-05,  6.79274035e-06,\n",
      "       -2.79942669e-05,  3.82934340e-05,  4.54702677e-05,  5.07088589e-06,\n",
      "       -1.99209262e-06, -2.74952472e-05, -1.11418296e-04,  1.60520329e-04,\n",
      "        2.97313527e-05, -7.68590598e-06, -1.26060941e-05,  1.60612835e-05,\n",
      "       -1.69430987e-05,  2.36206160e-05,  4.07348452e-05,  3.67538269e-05,\n",
      "       -2.08143956e-06,  9.88123938e-05, -8.13416118e-05,  3.15404795e-05,\n",
      "        6.68390494e-05, -2.03432937e-05, -3.43378197e-05,  1.85940153e-04,\n",
      "       -7.95384694e-05,  1.68244587e-05, -4.59674993e-07,  4.17957199e-05,\n",
      "       -1.69309205e-05, -1.40262709e-05,  7.96788390e-06, -2.78539446e-05,\n",
      "       -3.73238436e-05, -9.44947606e-05, -5.91418211e-05,  6.24326858e-05,\n",
      "       -9.93841313e-05,  2.89803174e-05, -2.24505711e-05, -9.23299285e-06,\n",
      "        5.38948298e-05, -1.37285824e-05, -1.92010884e-05,  1.17385271e-05,\n",
      "        3.62806531e-05, -8.11294303e-05, -1.02419999e-05,  1.47384389e-05,\n",
      "        2.50144058e-05, -1.12964481e-05,  3.63410800e-05,  1.01627647e-05,\n",
      "       -1.35937098e-05,  5.01271352e-05, -1.28543797e-05, -5.90891705e-06,\n",
      "       -3.05996618e-05,  1.68799834e-05,  1.58143357e-05, -2.72885754e-05,\n",
      "       -3.69055779e-05,  1.40685879e-04,  1.18622866e-05,  2.07124649e-05,\n",
      "       -5.95805832e-05, -6.46864864e-05,  1.09164275e-05,  7.91869024e-05,\n",
      "        5.55685692e-05, -1.75548994e-05,  1.10602869e-05, -5.14597632e-05,\n",
      "        6.76681011e-05, -3.90706737e-05,  4.52091554e-05, -8.88040086e-05,\n",
      "        1.32703590e-05, -1.57329268e-05,  2.43925642e-05,  6.42629857e-06,\n",
      "       -2.03525142e-05,  1.81547784e-05, -4.06847903e-05, -2.95622776e-05,\n",
      "       -1.33116773e-05,  1.48381132e-05, -2.03743075e-05, -5.50349341e-07,\n",
      "        1.39444892e-05,  4.89095473e-05,  4.44280658e-07,  5.22098999e-05,\n",
      "       -4.31941480e-05,  2.25880303e-05, -3.21573025e-05,  4.32889901e-05,\n",
      "        1.28165602e-05, -2.24725918e-05, -5.49294236e-05, -6.61755712e-06,\n",
      "        5.18650486e-05,  6.59520401e-06, -1.22958427e-05,  3.30729199e-05,\n",
      "        1.54675290e-05, -1.05692758e-04,  2.50157573e-05, -2.51526990e-06,\n",
      "       -3.03036422e-02,  4.05997038e-02,  4.92194593e-02,  9.73623991e-03,\n",
      "        2.69329399e-02,  2.08559614e-02,  4.98993732e-02,  2.37576831e-02,\n",
      "        1.29842386e-02, -3.41917900e-03, -1.54624144e-02,  5.98781556e-03,\n",
      "        2.96138804e-02,  6.18370250e-03,  2.51081889e-03, -3.52993119e-03,\n",
      "        1.66038908e-02,  2.18092762e-02,  7.46105611e-03, -7.33843446e-02,\n",
      "       -2.03893688e-02,  2.11062226e-02, -1.19402446e-02,  6.98949769e-03,\n",
      "        8.62122327e-03, -5.20788394e-02, -1.72211062e-02, -5.34097068e-02,\n",
      "        3.87233612e-03, -2.14230851e-03,  1.39879500e-02, -1.29462276e-02,\n",
      "        2.34022550e-02, -2.00851192e-03,  2.38693301e-02, -3.59786861e-02,\n",
      "       -2.75390223e-03, -4.10954133e-02,  3.97016108e-02, -7.79166771e-03,\n",
      "        3.50614451e-02,  5.90946386e-03,  5.37282750e-02, -7.09707439e-02,\n",
      "       -2.27573235e-03,  8.65102804e-04, -1.26714781e-02, -3.13489027e-02,\n",
      "       -9.45301354e-03,  1.80973504e-02, -3.40985581e-02, -2.44439226e-02,\n",
      "       -3.54438797e-02, -6.08896315e-02, -2.73492467e-02, -2.23763045e-02,\n",
      "        3.92201580e-02,  1.95835456e-02,  7.72279501e-03,  2.56498419e-02,\n",
      "       -2.57874262e-02, -5.09260595e-02, -2.86720181e-03, -1.32459346e-02,\n",
      "       -8.69037304e-03,  3.34079787e-02,  5.39310277e-03, -2.47038603e-02,\n",
      "       -2.86542997e-03, -6.35887962e-04, -2.85021383e-02, -4.90029459e-04,\n",
      "       -4.30005901e-02, -2.27625668e-02, -1.68900006e-02, -1.13418023e-03,\n",
      "        1.27519360e-02,  3.27897025e-03, -2.72600502e-02,  4.54128720e-03,\n",
      "       -3.01536117e-02,  5.09858876e-02, -1.01565672e-02, -1.40161738e-02,\n",
      "        4.56719622e-02,  2.28581224e-02,  3.36743742e-02,  6.22687191e-02,\n",
      "       -1.63156968e-02, -3.29495743e-02, -1.03638191e-02, -9.06650210e-04,\n",
      "       -9.12259705e-03, -2.78130397e-02,  1.92144904e-02,  4.87782508e-02,\n",
      "        1.73682496e-02,  3.28938551e-02, -1.11887639e-03,  1.67729687e-02,\n",
      "        7.03762332e-03, -1.63737964e-02, -2.81956457e-02,  3.28847878e-02,\n",
      "       -7.00910063e-03, -1.32448124e-02, -1.67460740e-02,  1.29921837e-02,\n",
      "       -7.29058450e-03,  4.18723784e-02,  8.57818872e-04, -3.48340236e-02,\n",
      "       -2.58192956e-03, -1.32221729e-02, -1.43669546e-02,  2.80862767e-03,\n",
      "        7.99112581e-03,  1.65172550e-03, -1.62892863e-02,  2.25257799e-02,\n",
      "        1.59411356e-02, -2.14355774e-02,  1.76051864e-03, -3.67775522e-02,\n",
      "       -7.41868280e-03,  4.93030697e-02,  4.63533401e-02,  3.09676193e-02,\n",
      "        2.40371774e-05,  6.61000922e-06, -7.79465772e-05,  6.04728484e-06,\n",
      "       -2.24296455e-05, -7.68595710e-05,  2.12915347e-06, -4.17622323e-05,\n",
      "       -3.73216899e-05, -2.55837358e-06, -3.27631351e-05, -2.80251606e-05,\n",
      "        1.52751236e-05,  7.52935885e-05,  2.06268705e-05,  3.51468516e-05,\n",
      "        4.24149148e-05,  2.98613304e-06, -1.66728660e-06,  2.29122052e-05,\n",
      "       -4.43060671e-05,  3.12800221e-05,  3.44945656e-05, -1.47085020e-05,\n",
      "        1.02346939e-05, -1.49794141e-05, -7.14722191e-05,  1.17274663e-04,\n",
      "       -2.84635698e-05,  1.07796022e-05,  3.14106001e-06,  8.52128960e-06,\n",
      "       -1.94792974e-05, -2.59022454e-06,  2.88766878e-05,  3.58075777e-05,\n",
      "       -1.34463698e-06,  5.92978831e-05, -7.30836182e-05,  2.15515229e-05,\n",
      "        1.07724718e-05, -4.50468178e-06, -1.86637553e-05,  1.27192761e-04,\n",
      "       -6.03124317e-05,  2.63713409e-05, -1.00943053e-05,  5.84233603e-05,\n",
      "        4.57230999e-06, -1.91531353e-06,  1.47665924e-05, -1.48168601e-06,\n",
      "       -1.59558222e-05, -6.35392571e-05, -5.37824017e-05,  5.41856571e-05,\n",
      "       -8.19555353e-05,  1.63244913e-05, -3.20729596e-05, -8.76334161e-06,\n",
      "        4.27253217e-05, -1.52116372e-05, -8.28961311e-06, -6.43690646e-06,\n",
      "        2.78877924e-05, -5.47744785e-05,  1.79838571e-05,  1.52480134e-05,\n",
      "        1.88518843e-05,  2.32102393e-05,  4.07172192e-05,  4.76170044e-06,\n",
      "       -1.94552331e-06,  3.11669573e-05, -2.18965924e-05, -1.10905512e-06,\n",
      "       -2.57267966e-05,  1.45722770e-05,  2.30399382e-05, -3.98161064e-05,\n",
      "       -1.52606844e-05,  8.44718015e-05,  3.24425091e-05,  1.71152496e-05,\n",
      "       -3.48145441e-05, -7.51387561e-05,  3.09478310e-07,  4.44301368e-05,\n",
      "        5.57573876e-05,  9.59218414e-06, -6.46663466e-06, -3.55285192e-05,\n",
      "        5.46127703e-05, -3.29813956e-05,  2.78179014e-05, -8.56274273e-05,\n",
      "        2.10855160e-06, -1.05297931e-05,  8.09213907e-06, -1.07283358e-06,\n",
      "       -3.78418008e-06,  2.57141655e-05, -6.68465145e-05, -2.56781623e-05,\n",
      "       -1.39472686e-05, -1.30790886e-06, -2.84556736e-05, -1.54482814e-06,\n",
      "        3.03851721e-06,  3.85292878e-05,  9.59471072e-06,  4.46915656e-05,\n",
      "       -1.53785386e-05,  3.33993921e-06, -1.65798010e-05,  1.59779102e-05,\n",
      "       -7.15280294e-06, -3.01333057e-05, -4.60913470e-05, -2.44623989e-05,\n",
      "        3.92555958e-05,  9.69419943e-06,  3.84444502e-06,  1.96928213e-05,\n",
      "        1.86361103e-06, -8.46302064e-05,  1.36410836e-05, -5.10618120e-05],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
      "array([[ 2.5502844e-03,  3.4643924e-03,  1.1333346e-03, ...,\n",
      "        -3.7310808e-06, -3.7294676e-06, -3.7297318e-06],\n",
      "       [ 3.6741432e-05,  8.6060655e-04, -3.5202847e-04, ...,\n",
      "         9.7511747e-07,  9.7519421e-07,  9.7686745e-07],\n",
      "       [ 1.8296725e-03,  3.4554347e-03,  2.0951871e-03, ...,\n",
      "        -4.7609783e-06, -4.7613225e-06, -4.7608041e-06],\n",
      "       ...,\n",
      "       [ 1.9483858e-03,  3.3519126e-03,  1.0845249e-03, ...,\n",
      "        -3.0708511e-06, -3.0688814e-06, -3.0703525e-06],\n",
      "       [ 4.2722505e-04, -6.0646475e-04,  2.8828933e-04, ...,\n",
      "         2.3097289e-07,  2.3376356e-07,  2.3380420e-07],\n",
      "       [-6.4099696e-04, -1.6094577e-03, -2.8885279e-03, ...,\n",
      "         2.2310585e-06,  2.2307211e-06,  2.2295415e-06]], dtype=float32)>, <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
      "array([-0.7979985 , -1.0313317 , -1.0313314 , ...,  0.0020002 ,\n",
      "        0.00199998,  0.00200016], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(grad_t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f050a80-9c9e-4375-9d49-f0998d047a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x1cce0fab1f0>,\n",
       " <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
       " array([[-1.21057360e-06,  1.11048939e-06,  1.81532386e-07, ...,\n",
       "         -9.68049108e-08, -3.67111710e-08, -2.76418064e-07],\n",
       "        [-5.07294317e-07,  1.44252951e-07, -3.60630679e-07, ...,\n",
       "          2.64155318e-07, -3.07675350e-07, -2.30252766e-07],\n",
       "        [ 7.60647367e-08,  4.53663773e-07,  2.49056058e-07, ...,\n",
       "          3.81014900e-08, -1.46729548e-07, -2.64645848e-07],\n",
       "        ...,\n",
       "        [-2.80405317e-07,  4.15165147e-08, -7.11877249e-07, ...,\n",
       "         -1.83229218e-07,  2.98460407e-07, -1.07527256e-07],\n",
       "        [-1.00541524e-06,  8.35447622e-09,  3.09439770e-07, ...,\n",
       "          3.70392456e-07,  8.74839259e-08, -1.75751911e-08],\n",
       "        [ 5.03668559e-07, -2.07654324e-07,  5.58799741e-07, ...,\n",
       "         -3.52775515e-08,  8.56571276e-08,  1.86711205e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
       " array([[-9.39754159e-08,  1.20600305e-07,  1.49682549e-07, ...,\n",
       "          7.05727476e-09,  1.08695993e-07, -5.68424241e-08],\n",
       "        [-1.25676607e-07, -2.38073767e-07,  1.11631017e-07, ...,\n",
       "         -8.36142675e-08,  5.11769187e-08,  2.35598208e-07],\n",
       "        [ 8.45096082e-08, -2.46632766e-07, -1.70544027e-07, ...,\n",
       "         -1.34838075e-07,  1.55118187e-07, -2.45559946e-08],\n",
       "        ...,\n",
       "        [ 8.54810196e-08, -2.42944964e-07, -5.53831434e-08, ...,\n",
       "          2.32535896e-07, -1.89823268e-08, -6.50923866e-08],\n",
       "        [-5.12402849e-08, -1.50184221e-09,  2.83389909e-07, ...,\n",
       "         -2.23845564e-08, -9.54893551e-08, -1.48459435e-07],\n",
       "        [-7.75154660e-08, -1.17515718e-07, -1.05618732e-07, ...,\n",
       "         -1.15903156e-07, -2.29969039e-08, -3.15501325e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
       " array([-5.38379027e-07,  1.60316449e-05, -3.12425982e-05, ...,\n",
       "        -7.08450443e-07,  9.99760960e-06,  1.23730915e-05], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
       " array([[ 6.06070785e-08,  6.87455923e-08,  2.43316549e-07, ...,\n",
       "          5.35215349e-07, -3.28748797e-08,  2.62320782e-07],\n",
       "        [ 3.66052689e-07, -2.42609701e-08, -2.37313643e-07, ...,\n",
       "         -3.54075610e-07, -5.13681009e-08,  2.19753176e-08],\n",
       "        [-1.98026875e-08,  9.22471060e-08, -3.30909074e-07, ...,\n",
       "         -4.02424860e-07,  6.62367938e-10, -1.78026710e-07],\n",
       "        ...,\n",
       "        [-1.71099188e-07,  7.63392194e-08,  3.08997471e-07, ...,\n",
       "          2.63024077e-07, -4.37670451e-08,  1.64783188e-07],\n",
       "        [ 5.45193828e-08,  9.91540077e-08,  1.17502260e-07, ...,\n",
       "          4.08629091e-07, -1.11947024e-07, -8.39211296e-08],\n",
       "        [-1.13241867e-07,  3.20164943e-08, -6.80008583e-08, ...,\n",
       "          1.40893292e-07, -8.66539622e-08,  1.48451704e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
       " array([[-2.83014316e-07,  1.87468714e-07,  1.28782503e-07, ...,\n",
       "          3.23095236e-07,  1.57001807e-08,  1.13150683e-07],\n",
       "        [ 7.05473440e-08,  4.75358178e-07, -1.00234580e-07, ...,\n",
       "         -6.71451801e-08,  9.61120747e-08,  2.60390998e-08],\n",
       "        [-1.18739919e-07, -5.65627190e-08,  4.37331437e-07, ...,\n",
       "          3.94034771e-07, -1.19753935e-07,  1.44101236e-07],\n",
       "        ...,\n",
       "        [-1.48003664e-07,  1.25589743e-08,  1.12538693e-07, ...,\n",
       "          8.71929615e-07,  4.78658535e-08,  1.13756116e-07],\n",
       "        [-5.59820563e-08, -2.40167680e-07,  3.46549811e-09, ...,\n",
       "          1.33972321e-07,  2.50653954e-07,  5.63789122e-08],\n",
       "        [ 1.24898776e-07, -4.92950925e-08,  3.87349459e-08, ...,\n",
       "          1.02594122e-07, -7.39265289e-08,  3.15293335e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
       " array([ 2.49469322e-05,  7.57677481e-06, -6.18866834e-05,  7.69003145e-06,\n",
       "        -2.01745715e-05, -7.07551080e-05,  2.34803110e-06, -3.37810634e-05,\n",
       "        -1.83314169e-05,  1.60549462e-07, -2.85602018e-05, -3.31013871e-05,\n",
       "         1.68080423e-05,  5.80545093e-05,  1.67938597e-05,  3.17677186e-05,\n",
       "         4.10325883e-05,  3.74359206e-06, -6.53401912e-07,  2.36837186e-05,\n",
       "        -3.87026193e-05,  2.80141649e-05,  3.28906317e-05, -6.07566471e-06,\n",
       "         7.30706779e-06, -1.52821212e-05, -6.35160395e-05,  9.47938097e-05,\n",
       "        -3.03235865e-05,  7.59836757e-06,  1.56651367e-06,  1.06875068e-05,\n",
       "        -1.54031641e-05, -3.18363982e-06,  2.58625005e-05,  4.22712765e-05,\n",
       "        -5.93205186e-06,  4.68410217e-05, -7.09049782e-05,  2.50994854e-05,\n",
       "         1.33377107e-05,  3.60924241e-06, -2.23639072e-05,  1.08682696e-04,\n",
       "        -5.17777808e-05,  2.59191656e-05, -4.63316701e-06,  4.23153142e-05,\n",
       "         5.39548546e-06,  2.84113571e-06,  1.48204444e-05,  2.45996398e-06,\n",
       "        -1.94262720e-05, -5.53641294e-05, -4.84403063e-05,  4.65652120e-05,\n",
       "        -6.85445339e-05,  2.23281277e-05, -2.61072819e-05, -1.11864410e-05,\n",
       "         2.71094577e-05, -1.33286103e-05, -9.72013822e-06, -1.57511931e-05,\n",
       "         2.42388487e-05, -5.52007186e-05,  4.92288382e-06,  1.43906227e-05,\n",
       "         1.09953053e-05,  2.31760569e-05,  3.18352395e-05,  9.57579960e-06,\n",
       "         1.34406309e-06,  3.28085080e-05, -1.70384956e-05, -3.86976080e-06,\n",
       "        -2.47961798e-05,  1.48823556e-05,  2.04836579e-05, -3.22020278e-05,\n",
       "        -1.08057520e-05,  7.62266864e-05,  2.59039571e-05,  1.29705195e-05,\n",
       "        -2.37094009e-05, -6.50939328e-05, -1.44419664e-06,  3.54880976e-05,\n",
       "         5.64080074e-05, -6.43202975e-06, -1.27343619e-05, -3.32722157e-05,\n",
       "         4.81199750e-05, -3.57114950e-05,  2.01606599e-05, -7.62829586e-05,\n",
       "         1.94226550e-06, -2.14329430e-05,  7.00861892e-06, -2.62525646e-07,\n",
       "        -5.66908966e-06,  2.22185663e-05, -5.50520563e-05, -3.14446806e-05,\n",
       "        -1.64065423e-05, -2.08175243e-06, -2.38712328e-05, -8.86377984e-06,\n",
       "         1.11705667e-05,  2.63177280e-05,  6.36487630e-06,  4.42090422e-05,\n",
       "        -1.72216387e-05,  8.35312221e-06, -1.43471034e-05,  1.11307145e-05,\n",
       "        -1.02987560e-05, -2.53891158e-05, -4.38444076e-05, -1.86000925e-05,\n",
       "         3.74551673e-05,  6.43037765e-06,  3.15131729e-07,  1.33504418e-05,\n",
       "         5.33794264e-06, -7.09733649e-05,  1.81977375e-05, -4.61492746e-05,\n",
       "         1.75672467e-05, -1.43281841e-05, -8.89812072e-05,  1.67135331e-05,\n",
       "        -6.21683212e-05, -1.09721732e-04, -6.30143859e-07, -4.53004468e-05,\n",
       "        -4.66381898e-05, -2.40990175e-07, -2.93775047e-05, -5.48587777e-05,\n",
       "         2.34140953e-05,  8.02775903e-05,  5.00582928e-06,  3.48928806e-05,\n",
       "         5.81128479e-05,  1.27842359e-05, -1.47568135e-05,  6.79274035e-06,\n",
       "        -2.79942669e-05,  3.82934340e-05,  4.54702677e-05,  5.07088589e-06,\n",
       "        -1.99209262e-06, -2.74952472e-05, -1.11418296e-04,  1.60520329e-04,\n",
       "         2.97313527e-05, -7.68590598e-06, -1.26060941e-05,  1.60612835e-05,\n",
       "        -1.69430987e-05,  2.36206160e-05,  4.07348452e-05,  3.67538269e-05,\n",
       "        -2.08143956e-06,  9.88123938e-05, -8.13416118e-05,  3.15404795e-05,\n",
       "         6.68390494e-05, -2.03432937e-05, -3.43378197e-05,  1.85940153e-04,\n",
       "        -7.95384694e-05,  1.68244587e-05, -4.59674993e-07,  4.17957199e-05,\n",
       "        -1.69309205e-05, -1.40262709e-05,  7.96788390e-06, -2.78539446e-05,\n",
       "        -3.73238436e-05, -9.44947606e-05, -5.91418211e-05,  6.24326858e-05,\n",
       "        -9.93841313e-05,  2.89803174e-05, -2.24505711e-05, -9.23299285e-06,\n",
       "         5.38948298e-05, -1.37285824e-05, -1.92010884e-05,  1.17385271e-05,\n",
       "         3.62806531e-05, -8.11294303e-05, -1.02419999e-05,  1.47384389e-05,\n",
       "         2.50144058e-05, -1.12964481e-05,  3.63410800e-05,  1.01627647e-05,\n",
       "        -1.35937098e-05,  5.01271352e-05, -1.28543797e-05, -5.90891705e-06,\n",
       "        -3.05996618e-05,  1.68799834e-05,  1.58143357e-05, -2.72885754e-05,\n",
       "        -3.69055779e-05,  1.40685879e-04,  1.18622866e-05,  2.07124649e-05,\n",
       "        -5.95805832e-05, -6.46864864e-05,  1.09164275e-05,  7.91869024e-05,\n",
       "         5.55685692e-05, -1.75548994e-05,  1.10602869e-05, -5.14597632e-05,\n",
       "         6.76681011e-05, -3.90706737e-05,  4.52091554e-05, -8.88040086e-05,\n",
       "         1.32703590e-05, -1.57329268e-05,  2.43925642e-05,  6.42629857e-06,\n",
       "        -2.03525142e-05,  1.81547784e-05, -4.06847903e-05, -2.95622776e-05,\n",
       "        -1.33116773e-05,  1.48381132e-05, -2.03743075e-05, -5.50349341e-07,\n",
       "         1.39444892e-05,  4.89095473e-05,  4.44280658e-07,  5.22098999e-05,\n",
       "        -4.31941480e-05,  2.25880303e-05, -3.21573025e-05,  4.32889901e-05,\n",
       "         1.28165602e-05, -2.24725918e-05, -5.49294236e-05, -6.61755712e-06,\n",
       "         5.18650486e-05,  6.59520401e-06, -1.22958427e-05,  3.30729199e-05,\n",
       "         1.54675290e-05, -1.05692758e-04,  2.50157573e-05, -2.51526990e-06,\n",
       "        -3.03036422e-02,  4.05997038e-02,  4.92194593e-02,  9.73623991e-03,\n",
       "         2.69329399e-02,  2.08559614e-02,  4.98993732e-02,  2.37576831e-02,\n",
       "         1.29842386e-02, -3.41917900e-03, -1.54624144e-02,  5.98781556e-03,\n",
       "         2.96138804e-02,  6.18370250e-03,  2.51081889e-03, -3.52993119e-03,\n",
       "         1.66038908e-02,  2.18092762e-02,  7.46105611e-03, -7.33843446e-02,\n",
       "        -2.03893688e-02,  2.11062226e-02, -1.19402446e-02,  6.98949769e-03,\n",
       "         8.62122327e-03, -5.20788394e-02, -1.72211062e-02, -5.34097068e-02,\n",
       "         3.87233612e-03, -2.14230851e-03,  1.39879500e-02, -1.29462276e-02,\n",
       "         2.34022550e-02, -2.00851192e-03,  2.38693301e-02, -3.59786861e-02,\n",
       "        -2.75390223e-03, -4.10954133e-02,  3.97016108e-02, -7.79166771e-03,\n",
       "         3.50614451e-02,  5.90946386e-03,  5.37282750e-02, -7.09707439e-02,\n",
       "        -2.27573235e-03,  8.65102804e-04, -1.26714781e-02, -3.13489027e-02,\n",
       "        -9.45301354e-03,  1.80973504e-02, -3.40985581e-02, -2.44439226e-02,\n",
       "        -3.54438797e-02, -6.08896315e-02, -2.73492467e-02, -2.23763045e-02,\n",
       "         3.92201580e-02,  1.95835456e-02,  7.72279501e-03,  2.56498419e-02,\n",
       "        -2.57874262e-02, -5.09260595e-02, -2.86720181e-03, -1.32459346e-02,\n",
       "        -8.69037304e-03,  3.34079787e-02,  5.39310277e-03, -2.47038603e-02,\n",
       "        -2.86542997e-03, -6.35887962e-04, -2.85021383e-02, -4.90029459e-04,\n",
       "        -4.30005901e-02, -2.27625668e-02, -1.68900006e-02, -1.13418023e-03,\n",
       "         1.27519360e-02,  3.27897025e-03, -2.72600502e-02,  4.54128720e-03,\n",
       "        -3.01536117e-02,  5.09858876e-02, -1.01565672e-02, -1.40161738e-02,\n",
       "         4.56719622e-02,  2.28581224e-02,  3.36743742e-02,  6.22687191e-02,\n",
       "        -1.63156968e-02, -3.29495743e-02, -1.03638191e-02, -9.06650210e-04,\n",
       "        -9.12259705e-03, -2.78130397e-02,  1.92144904e-02,  4.87782508e-02,\n",
       "         1.73682496e-02,  3.28938551e-02, -1.11887639e-03,  1.67729687e-02,\n",
       "         7.03762332e-03, -1.63737964e-02, -2.81956457e-02,  3.28847878e-02,\n",
       "        -7.00910063e-03, -1.32448124e-02, -1.67460740e-02,  1.29921837e-02,\n",
       "        -7.29058450e-03,  4.18723784e-02,  8.57818872e-04, -3.48340236e-02,\n",
       "        -2.58192956e-03, -1.32221729e-02, -1.43669546e-02,  2.80862767e-03,\n",
       "         7.99112581e-03,  1.65172550e-03, -1.62892863e-02,  2.25257799e-02,\n",
       "         1.59411356e-02, -2.14355774e-02,  1.76051864e-03, -3.67775522e-02,\n",
       "        -7.41868280e-03,  4.93030697e-02,  4.63533401e-02,  3.09676193e-02,\n",
       "         2.40371774e-05,  6.61000922e-06, -7.79465772e-05,  6.04728484e-06,\n",
       "        -2.24296455e-05, -7.68595710e-05,  2.12915347e-06, -4.17622323e-05,\n",
       "        -3.73216899e-05, -2.55837358e-06, -3.27631351e-05, -2.80251606e-05,\n",
       "         1.52751236e-05,  7.52935885e-05,  2.06268705e-05,  3.51468516e-05,\n",
       "         4.24149148e-05,  2.98613304e-06, -1.66728660e-06,  2.29122052e-05,\n",
       "        -4.43060671e-05,  3.12800221e-05,  3.44945656e-05, -1.47085020e-05,\n",
       "         1.02346939e-05, -1.49794141e-05, -7.14722191e-05,  1.17274663e-04,\n",
       "        -2.84635698e-05,  1.07796022e-05,  3.14106001e-06,  8.52128960e-06,\n",
       "        -1.94792974e-05, -2.59022454e-06,  2.88766878e-05,  3.58075777e-05,\n",
       "        -1.34463698e-06,  5.92978831e-05, -7.30836182e-05,  2.15515229e-05,\n",
       "         1.07724718e-05, -4.50468178e-06, -1.86637553e-05,  1.27192761e-04,\n",
       "        -6.03124317e-05,  2.63713409e-05, -1.00943053e-05,  5.84233603e-05,\n",
       "         4.57230999e-06, -1.91531353e-06,  1.47665924e-05, -1.48168601e-06,\n",
       "        -1.59558222e-05, -6.35392571e-05, -5.37824017e-05,  5.41856571e-05,\n",
       "        -8.19555353e-05,  1.63244913e-05, -3.20729596e-05, -8.76334161e-06,\n",
       "         4.27253217e-05, -1.52116372e-05, -8.28961311e-06, -6.43690646e-06,\n",
       "         2.78877924e-05, -5.47744785e-05,  1.79838571e-05,  1.52480134e-05,\n",
       "         1.88518843e-05,  2.32102393e-05,  4.07172192e-05,  4.76170044e-06,\n",
       "        -1.94552331e-06,  3.11669573e-05, -2.18965924e-05, -1.10905512e-06,\n",
       "        -2.57267966e-05,  1.45722770e-05,  2.30399382e-05, -3.98161064e-05,\n",
       "        -1.52606844e-05,  8.44718015e-05,  3.24425091e-05,  1.71152496e-05,\n",
       "        -3.48145441e-05, -7.51387561e-05,  3.09478310e-07,  4.44301368e-05,\n",
       "         5.57573876e-05,  9.59218414e-06, -6.46663466e-06, -3.55285192e-05,\n",
       "         5.46127703e-05, -3.29813956e-05,  2.78179014e-05, -8.56274273e-05,\n",
       "         2.10855160e-06, -1.05297931e-05,  8.09213907e-06, -1.07283358e-06,\n",
       "        -3.78418008e-06,  2.57141655e-05, -6.68465145e-05, -2.56781623e-05,\n",
       "        -1.39472686e-05, -1.30790886e-06, -2.84556736e-05, -1.54482814e-06,\n",
       "         3.03851721e-06,  3.85292878e-05,  9.59471072e-06,  4.46915656e-05,\n",
       "        -1.53785386e-05,  3.33993921e-06, -1.65798010e-05,  1.59779102e-05,\n",
       "        -7.15280294e-06, -3.01333057e-05, -4.60913470e-05, -2.44623989e-05,\n",
       "         3.92555958e-05,  9.69419943e-06,  3.84444502e-06,  1.96928213e-05,\n",
       "         1.86361103e-06, -8.46302064e-05,  1.36410836e-05, -5.10618120e-05],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
       " array([[ 2.5502844e-03,  3.4643924e-03,  1.1333346e-03, ...,\n",
       "         -3.7310808e-06, -3.7294676e-06, -3.7297318e-06],\n",
       "        [ 3.6741432e-05,  8.6060655e-04, -3.5202847e-04, ...,\n",
       "          9.7511747e-07,  9.7519421e-07,  9.7686745e-07],\n",
       "        [ 1.8296725e-03,  3.4554347e-03,  2.0951871e-03, ...,\n",
       "         -4.7609783e-06, -4.7613225e-06, -4.7608041e-06],\n",
       "        ...,\n",
       "        [ 1.9483858e-03,  3.3519126e-03,  1.0845249e-03, ...,\n",
       "         -3.0708511e-06, -3.0688814e-06, -3.0703525e-06],\n",
       "        [ 4.2722505e-04, -6.0646475e-04,  2.8828933e-04, ...,\n",
       "          2.3097289e-07,  2.3376356e-07,  2.3380420e-07],\n",
       "        [-6.4099696e-04, -1.6094577e-03, -2.8885279e-03, ...,\n",
       "          2.2310585e-06,  2.2307211e-06,  2.2295415e-06]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
       " array([-0.7979985 , -1.0313317 , -1.0313314 , ...,  0.0020002 ,\n",
       "         0.00199998,  0.00200016], dtype=float32)>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80bf7de9-06e7-436e-9b56-7ab592658aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "365e4855-375d-4da1-8180-16b9a6be1456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = 1.0\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Initializing the model using keras Sequential API  #\n",
    "        ###############################################################################\n",
    "        \n",
    "        self._model = tf.keras.models.Sequential()\n",
    "        \n",
    "        ####################################################################\n",
    "        # Creating the word embeddings layer and adding it to the sequence #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\n",
    "            self._model.add(self._embedding_layer)\n",
    "            \n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM Cells. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A). \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "        lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
    "        \n",
    "\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of stacked simple cells.\n",
    "        stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ########################################################################################################\n",
    "        # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\n",
    "        ########################################################################################################\n",
    "        \n",
    "        self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\n",
    "        \n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
    "        self._RNNlayer.inital_state = self._initial_state\n",
    "    \n",
    "        ############################################\n",
    "        # Adding RNN layer to keras sequential API #\n",
    "        ############################################        \n",
    "        self._model.add(self._RNNlayer)\n",
    "        \n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\n",
    "        \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\n",
    "        ####################################################################################################\n",
    "        self._dense = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self._model.add(self._dense)\n",
    " \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Adding softmax activation layer and deriving probability to each class and adding layer to model #\n",
    "        ####################################################################################################\n",
    "        self._activation = tf.keras.layers.Activation('softmax')\n",
    "        self._model.add(self._activation)\n",
    "\n",
    "        ##########################################################\n",
    "        # Instantiating the stochastic gradient decent optimizer #\n",
    "        ########################################################## \n",
    "        self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        # Compiling and summarizing the model stacked using the keras sequential API #\n",
    "        ##############################################################################\n",
    "        self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\n",
    "        self._model.summary()\n",
    "\n",
    "\n",
    "    def crossentropy(self,y_true, y_pred):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def train_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = self._model.trainable_variables\n",
    "        # Define the gradient clipping threshold\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass.\n",
    "            output_words_prob = self._model(_input_data)\n",
    "            # Loss value for this batch.\n",
    "            loss  = self.crossentropy(_targets, output_words_prob)\n",
    "            # average across batch and reduce sum\n",
    "            cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "        # Get gradients of loss wrt the trainable variables.\n",
    "        grad_t_list = tape.gradient(cost, tvars)\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return cost\n",
    "        \n",
    "    def test_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Testing Operation for our Model #\n",
    "        #################################################\n",
    "        output_words_prob = self._model(_input_data)\n",
    "        loss  = self.crossentropy(_targets, output_words_prob)\n",
    "        # average across batch and reduce sum\n",
    "        cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "\n",
    "        return cost\n",
    "    @classmethod\n",
    "    def instance(cls) : \n",
    "        return PTBModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1017d6d9-0abe-4c5c-afd0-327bbd1b6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\n",
    "########################################################################################################################\n",
    "def run_one_epoch(m, data,is_training=True,verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.\n",
    "    iters = 0\n",
    "    \n",
    "    m._model.reset_states()\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "        if is_training : \n",
    "            loss=  m.train_batch(x, y)\n",
    "        else :\n",
    "            loss = m.test_batch(x, y)\n",
    "                                   \n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += loss\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "        \n",
    "\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87f150c0-e1f5-4fe0-8b93-34e7e9944037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "931457b0-b585-428a-ac33-a96f39080eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_vocab (Embedding)  (30, 20, 200)            2000000   \n",
      "                                                                 \n",
      " rnn_2 (RNN)                 (30, 20, 128)             671088    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (30, 20, 10000)           1290000   \n",
      "                                                                 \n",
      " activation_1 (Activation)   (30, 20, 10000)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 4592.739 speed: 2376 wps\n",
      "Itr 164 of 1549, perplexity: 1096.940 speed: 2583 wps\n",
      "Itr 318 of 1549, perplexity: 845.598 speed: 2680 wps\n",
      "Itr 472 of 1549, perplexity: 701.067 speed: 2805 wps\n",
      "Itr 626 of 1549, perplexity: 596.454 speed: 2839 wps\n",
      "Itr 780 of 1549, perplexity: 529.364 speed: 2865 wps\n",
      "Itr 934 of 1549, perplexity: 477.059 speed: 2888 wps\n",
      "Itr 1088 of 1549, perplexity: 437.850 speed: 2906 wps\n",
      "Itr 1242 of 1549, perplexity: 407.801 speed: 2924 wps\n",
      "Itr 1396 of 1549, perplexity: 379.943 speed: 2925 wps\n",
      "Epoch 1 : Train Perplexity: 357.922\n",
      "Epoch 1 : Valid Perplexity: 211.745\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 234.166 speed: 2930 wps\n",
      "Itr 164 of 1549, perplexity: 211.154 speed: 2921 wps\n",
      "Itr 318 of 1549, perplexity: 202.718 speed: 2960 wps\n",
      "Itr 472 of 1549, perplexity: 194.279 speed: 2948 wps\n",
      "Itr 626 of 1549, perplexity: 185.417 speed: 2939 wps\n",
      "Itr 780 of 1549, perplexity: 181.731 speed: 2902 wps\n",
      "Itr 934 of 1549, perplexity: 177.749 speed: 2898 wps\n",
      "Itr 1088 of 1549, perplexity: 174.372 speed: 2885 wps\n",
      "Itr 1242 of 1549, perplexity: 171.988 speed: 2904 wps\n",
      "Itr 1396 of 1549, perplexity: 167.909 speed: 2863 wps\n",
      "Epoch 2 : Train Perplexity: 165.068\n",
      "Epoch 2 : Valid Perplexity: 162.919\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 165.299 speed: 2830 wps\n",
      "Itr 164 of 1549, perplexity: 148.336 speed: 2680 wps\n",
      "Itr 318 of 1549, perplexity: 144.809 speed: 2735 wps\n",
      "Itr 472 of 1549, perplexity: 140.321 speed: 2776 wps\n",
      "Itr 626 of 1549, perplexity: 135.268 speed: 2732 wps\n",
      "Itr 780 of 1549, perplexity: 134.025 speed: 2751 wps\n",
      "Itr 934 of 1549, perplexity: 132.279 speed: 2678 wps\n",
      "Itr 1088 of 1549, perplexity: 130.747 speed: 2647 wps\n",
      "Itr 1242 of 1549, perplexity: 129.859 speed: 2665 wps\n",
      "Itr 1396 of 1549, perplexity: 127.502 speed: 2632 wps\n",
      "Epoch 3 : Train Perplexity: 126.147\n",
      "Epoch 3 : Valid Perplexity: 144.932\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 133.575 speed: 2620 wps\n",
      "Itr 164 of 1549, perplexity: 121.309 speed: 2762 wps\n",
      "Itr 318 of 1549, perplexity: 119.236 speed: 2742 wps\n",
      "Itr 472 of 1549, perplexity: 115.890 speed: 2773 wps\n",
      "Itr 626 of 1549, perplexity: 112.132 speed: 2761 wps\n",
      "Itr 780 of 1549, perplexity: 111.562 speed: 2770 wps\n",
      "Itr 934 of 1549, perplexity: 110.522 speed: 2769 wps\n",
      "Itr 1088 of 1549, perplexity: 109.582 speed: 2774 wps\n",
      "Itr 1242 of 1549, perplexity: 109.159 speed: 2796 wps\n",
      "Itr 1396 of 1549, perplexity: 107.453 speed: 2790 wps\n",
      "Epoch 4 : Train Perplexity: 106.631\n",
      "Epoch 4 : Valid Perplexity: 138.165\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 114.023 speed: 2755 wps\n",
      "Itr 164 of 1549, perplexity: 105.698 speed: 2831 wps\n",
      "Itr 318 of 1549, perplexity: 104.290 speed: 2896 wps\n",
      "Itr 472 of 1549, perplexity: 101.452 speed: 2917 wps\n",
      "Itr 626 of 1549, perplexity: 98.257 speed: 2888 wps\n",
      "Itr 780 of 1549, perplexity: 97.978 speed: 2878 wps\n",
      "Itr 934 of 1549, perplexity: 97.294 speed: 2872 wps\n",
      "Itr 1088 of 1549, perplexity: 96.654 speed: 2880 wps\n",
      "Itr 1242 of 1549, perplexity: 96.436 speed: 2851 wps\n",
      "Itr 1396 of 1549, perplexity: 95.068 speed: 2848 wps\n",
      "Epoch 5 : Train Perplexity: 94.511\n",
      "Epoch 5 : Valid Perplexity: 133.636\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr 10 of 1549, perplexity: 101.156 speed: 2635 wps\n",
      "Itr 164 of 1549, perplexity: 91.982 speed: 2797 wps\n",
      "Itr 318 of 1549, perplexity: 89.399 speed: 2751 wps\n",
      "Itr 472 of 1549, perplexity: 86.034 speed: 2871 wps\n",
      "Itr 626 of 1549, perplexity: 82.484 speed: 2935 wps\n",
      "Itr 780 of 1549, perplexity: 81.647 speed: 2980 wps\n",
      "Itr 934 of 1549, perplexity: 80.469 speed: 3011 wps\n",
      "Itr 1088 of 1549, perplexity: 79.355 speed: 3028 wps\n",
      "Itr 1242 of 1549, perplexity: 78.578 speed: 3045 wps\n",
      "Itr 1396 of 1549, perplexity: 76.874 speed: 3060 wps\n",
      "Epoch 6 : Train Perplexity: 75.876\n",
      "Epoch 6 : Valid Perplexity: 124.616\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "Itr 10 of 1549, perplexity: 84.871 speed: 3161 wps\n",
      "Itr 164 of 1549, perplexity: 79.038 speed: 3164 wps\n",
      "Itr 318 of 1549, perplexity: 77.021 speed: 3167 wps\n",
      "Itr 472 of 1549, perplexity: 74.014 speed: 3161 wps\n",
      "Itr 626 of 1549, perplexity: 70.896 speed: 3166 wps\n",
      "Itr 780 of 1549, perplexity: 70.114 speed: 3160 wps\n",
      "Itr 934 of 1549, perplexity: 69.043 speed: 3158 wps\n",
      "Itr 1088 of 1549, perplexity: 67.966 speed: 3153 wps\n",
      "Itr 1242 of 1549, perplexity: 67.159 speed: 3151 wps\n",
      "Itr 1396 of 1549, perplexity: 65.534 speed: 3151 wps\n",
      "Epoch 7 : Train Perplexity: 64.517\n",
      "Epoch 7 : Valid Perplexity: 122.480\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "Itr 10 of 1549, perplexity: 76.598 speed: 3126 wps\n",
      "Itr 164 of 1549, perplexity: 71.721 speed: 3144 wps\n",
      "Itr 318 of 1549, perplexity: 70.028 speed: 3161 wps\n",
      "Itr 472 of 1549, perplexity: 67.342 speed: 3174 wps\n",
      "Itr 626 of 1549, perplexity: 64.467 speed: 3173 wps\n",
      "Itr 780 of 1549, perplexity: 63.763 speed: 3177 wps\n",
      "Itr 934 of 1549, perplexity: 62.771 speed: 3180 wps\n",
      "Itr 1088 of 1549, perplexity: 61.745 speed: 3184 wps\n",
      "Itr 1242 of 1549, perplexity: 60.969 speed: 3186 wps\n",
      "Itr 1396 of 1549, perplexity: 59.438 speed: 3187 wps\n",
      "Epoch 8 : Train Perplexity: 58.456\n",
      "Epoch 8 : Valid Perplexity: 122.123\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "Itr 10 of 1549, perplexity: 72.419 speed: 3099 wps\n",
      "Itr 164 of 1549, perplexity: 67.919 speed: 3162 wps\n",
      "Itr 318 of 1549, perplexity: 66.406 speed: 3175 wps\n",
      "Itr 472 of 1549, perplexity: 63.899 speed: 3171 wps\n",
      "Itr 626 of 1549, perplexity: 61.143 speed: 3177 wps\n",
      "Itr 780 of 1549, perplexity: 60.474 speed: 3178 wps\n",
      "Itr 934 of 1549, perplexity: 59.534 speed: 3175 wps\n",
      "Itr 1088 of 1549, perplexity: 58.532 speed: 3178 wps\n",
      "Itr 1242 of 1549, perplexity: 57.769 speed: 3179 wps\n",
      "Itr 1396 of 1549, perplexity: 56.296 speed: 3180 wps\n",
      "Epoch 9 : Train Perplexity: 55.333\n",
      "Epoch 9 : Valid Perplexity: 121.962\n",
      "Epoch 10 : Learning rate: 0.031\n",
      "Itr 10 of 1549, perplexity: 70.241 speed: 3106 wps\n",
      "Itr 164 of 1549, perplexity: 65.882 speed: 3199 wps\n",
      "Itr 318 of 1549, perplexity: 64.456 speed: 3184 wps\n",
      "Itr 472 of 1549, perplexity: 62.068 speed: 3185 wps\n",
      "Itr 626 of 1549, perplexity: 59.380 speed: 3190 wps\n",
      "Itr 780 of 1549, perplexity: 58.725 speed: 3192 wps\n",
      "Itr 934 of 1549, perplexity: 57.819 speed: 3188 wps\n",
      "Itr 1088 of 1549, perplexity: 56.827 speed: 3182 wps\n",
      "Itr 1242 of 1549, perplexity: 56.065 speed: 3153 wps\n",
      "Itr 1396 of 1549, perplexity: 54.621 speed: 3113 wps\n",
      "Epoch 10 : Train Perplexity: 53.665\n",
      "Epoch 10 : Valid Perplexity: 121.964\n",
      "Epoch 11 : Learning rate: 0.016\n",
      "Itr 10 of 1549, perplexity: 68.986 speed: 3058 wps\n",
      "Itr 164 of 1549, perplexity: 64.779 speed: 3063 wps\n",
      "Itr 318 of 1549, perplexity: 63.391 speed: 3010 wps\n",
      "Itr 472 of 1549, perplexity: 61.055 speed: 3042 wps\n",
      "Itr 626 of 1549, perplexity: 58.415 speed: 3075 wps\n",
      "Itr 780 of 1549, perplexity: 57.769 speed: 3090 wps\n",
      "Itr 934 of 1549, perplexity: 56.882 speed: 3100 wps\n",
      "Itr 1088 of 1549, perplexity: 55.894 speed: 3109 wps\n",
      "Itr 1242 of 1549, perplexity: 55.132 speed: 3103 wps\n",
      "Itr 1396 of 1549, perplexity: 53.705 speed: 3113 wps\n",
      "Epoch 11 : Train Perplexity: 52.751\n",
      "Epoch 11 : Valid Perplexity: 121.873\n",
      "Epoch 12 : Learning rate: 0.008\n",
      "Itr 10 of 1549, perplexity: 68.224 speed: 3092 wps\n",
      "Itr 164 of 1549, perplexity: 64.167 speed: 3186 wps\n",
      "Itr 318 of 1549, perplexity: 62.809 speed: 3193 wps\n",
      "Itr 472 of 1549, perplexity: 60.495 speed: 3196 wps\n",
      "Itr 626 of 1549, perplexity: 57.878 speed: 3197 wps\n",
      "Itr 780 of 1549, perplexity: 57.241 speed: 3203 wps\n",
      "Itr 934 of 1549, perplexity: 56.364 speed: 3201 wps\n",
      "Itr 1088 of 1549, perplexity: 55.381 speed: 3204 wps\n",
      "Itr 1242 of 1549, perplexity: 54.619 speed: 3203 wps\n",
      "Itr 1396 of 1549, perplexity: 53.204 speed: 3203 wps\n",
      "Epoch 12 : Train Perplexity: 52.251\n",
      "Epoch 12 : Valid Perplexity: 121.683\n",
      "Epoch 13 : Learning rate: 0.004\n",
      "Itr 10 of 1549, perplexity: 67.797 speed: 3061 wps\n",
      "Itr 164 of 1549, perplexity: 63.823 speed: 3192 wps\n",
      "Itr 318 of 1549, perplexity: 62.491 speed: 3197 wps\n",
      "Itr 472 of 1549, perplexity: 60.191 speed: 3201 wps\n",
      "Itr 626 of 1549, perplexity: 57.586 speed: 3209 wps\n",
      "Itr 780 of 1549, perplexity: 56.952 speed: 3210 wps\n",
      "Itr 934 of 1549, perplexity: 56.081 speed: 3211 wps\n",
      "Itr 1088 of 1549, perplexity: 55.103 speed: 3211 wps\n",
      "Itr 1242 of 1549, perplexity: 54.341 speed: 3210 wps\n",
      "Itr 1396 of 1549, perplexity: 52.932 speed: 3199 wps\n",
      "Epoch 13 : Train Perplexity: 51.982\n",
      "Epoch 13 : Valid Perplexity: 121.483\n",
      "Epoch 14 : Learning rate: 0.002\n",
      "Itr 10 of 1549, perplexity: 67.562 speed: 3127 wps\n",
      "Itr 164 of 1549, perplexity: 63.627 speed: 3204 wps\n",
      "Itr 318 of 1549, perplexity: 62.312 speed: 3206 wps\n",
      "Itr 472 of 1549, perplexity: 60.027 speed: 3205 wps\n",
      "Itr 626 of 1549, perplexity: 57.428 speed: 3203 wps\n",
      "Itr 780 of 1549, perplexity: 56.796 speed: 3200 wps\n",
      "Itr 934 of 1549, perplexity: 55.929 speed: 3201 wps\n",
      "Itr 1088 of 1549, perplexity: 54.954 speed: 3200 wps\n",
      "Itr 1242 of 1549, perplexity: 54.193 speed: 3199 wps\n",
      "Itr 1396 of 1549, perplexity: 52.788 speed: 3198 wps\n",
      "Epoch 14 : Train Perplexity: 51.840\n",
      "Epoch 14 : Valid Perplexity: 121.330\n",
      "Epoch 15 : Learning rate: 0.001\n",
      "Itr 10 of 1549, perplexity: 67.441 speed: 3142 wps\n",
      "Itr 164 of 1549, perplexity: 63.515 speed: 3183 wps\n",
      "Itr 318 of 1549, perplexity: 62.209 speed: 3188 wps\n",
      "Itr 472 of 1549, perplexity: 59.936 speed: 3187 wps\n",
      "Itr 626 of 1549, perplexity: 57.342 speed: 3191 wps\n",
      "Itr 780 of 1549, perplexity: 56.712 speed: 3187 wps\n",
      "Itr 934 of 1549, perplexity: 55.848 speed: 3189 wps\n",
      "Itr 1088 of 1549, perplexity: 54.875 speed: 3190 wps\n",
      "Itr 1242 of 1549, perplexity: 54.114 speed: 3190 wps\n",
      "Itr 1396 of 1549, perplexity: 52.711 speed: 3190 wps\n",
      "Epoch 15 : Train Perplexity: 51.765\n",
      "Epoch 15 : Valid Perplexity: 121.245\n",
      "Test Perplexity: 115.854\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the PTBModel class\n",
    "m=PTBModel.instance()   \n",
    "K = tf.keras.backend \n",
    "for i in range(max_epoch):\n",
    "    # Define the decay for this epoch\n",
    "    lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "    dcr = learning_rate * lr_decay\n",
    "    m._lr = dcr\n",
    "    K.set_value(m._model.optimizer.learning_rate,m._lr)\n",
    "    print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\n",
    "    # Run the loop for this epoch in the training mode\n",
    "    train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\n",
    "    print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "    # Run the loop for this epoch in the validation mode\n",
    "    valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\n",
    "    print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "# Run the loop in the testing mode to see how effective was our training\n",
    "test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\n",
    "print(\"Test Perplexity: %.3f\" % test_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4730388-2cc0-4038-9a25-48add48af4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
